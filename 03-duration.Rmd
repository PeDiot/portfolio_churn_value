---
output: html_document2
editor_options: 
  chunk_output_type: console
---

```{r echo=FALSE}
knitr::opts_chunk$set(
  fig.align = "center", 
  echo = FALSE, 
  message = FALSE, 
  comment = FALSE, 
  warning = FALSE
)
```

```{r packages}
library(ggplot2)
library(ggpubr)
library(tidyverse)
library(latex2exp)
```

```{r plot_config}
theme_set(theme_minimal())
plot_col <- "#4DB9A5"
```


# Duration models {#duration}

This chapter presents theoretical basis of the models that are used to model customer portfolios. As a customer's lifetime in a portfolio is usually represented by the time to churn, duration models are adapted to the data we have at our disposal. Thus, this part focuses on introducing standard survival techniques. 

## Definition

According to Cameron & Trivedi [@CAMERON_TRIVEDI], duration models (also called survival models) aims at measuring the time spent in a certain state before transitioning to another state. In Econometrics, 

- a **state** corresponds to the class in which an individual $i$ is at time $t$. 
- a **transition** is movement from one state to another. 
- a **duration** measures the time spent in a certain state and is also called a **spell** length. 

Since measuring the time until the event is needed for multiple purposes, duration analysis is used in a variety of economic sectors as depicted by the following table. 

| Economic sector    | Purpose                                       | 
| :---------------:  |:--------------------------------------------: | 
| Macroeconomics     | Length of an unemployment spell               | 
| Insurance          | Risk analysis to offer a segmented pricing    | 
| Engineering        | Time until a device breaks down               |
| Epidemiology       | Survival time of a virus                      | 
| **Churn analysis** | **Time until a customer leave the portfolio** |


## Censoring and Truncation

When dealing with survival data, some observations are usually **censored** meaning they are related to spells which are not completely observed. Duration data can also suffer from a selection bias which is called **truncation**. 

### Censoring mechanisms

**Left-censoring** occurs when the event of interest occurs before the beginning of the observation period. For example, an individual is included in a study of duration of unemployment at $t_0$. At that time he has already been unemployed for a period but he cannot recall exactly the duration of this period. If we observe that he finds a job again at $t_1$, we can only deduce that the duration of unemployment is bigger than $t_1-t_0$, this individual is left-censored. Observation 2 on figure \@ref(fig:censoring) is associated with a left-censored spell [@LIU_SCOR].  

A spell is considered **right-censored** when it is observed from time $t_0$ until a censoring time $t_c$ as illustrated by observation 1 on figure \@ref(fig:censoring). For instance, the lifetime related to a customer who has not churned at the end of the observation period is right-censored. Let us note $X_i$ the duration of a complete spell and $C_i$ the duration of a right-censored spell. We also note $T_i$ the duration actually observed and $\delta_i$ the censoring indicator such that $\delta_i = 0$ if the spell is censored. Then, $(t_1, \delta_1),\dots,(t_N, \delta_N)$ are the realizations of the following random variables: 

\begin{equation}
  \begin{aligned}
  T_i & = \min(X_i, C_i) \\
  \delta_i & = \mathbb{1}_{X_i < C_i}
  \end{aligned}
  (\#eq:censoring)
\end{equation}

### Selection bias

Survival data suffers from a **selection bias** (or truncation) when only a sub-sample of the population of interest is studied. A customer entering the firm's portfolio after the end of the study is said to be **right-truncated**, whereas a client who has left the portfolio before the beginning of the study is considered **left-truncated**. Mathematically, a random variable $X$ is truncated by a subset $A \in \mathbb{R}^+$ if instead of $\Omega(X)$, we solely observe $\Omega(X)\bigcap A$. On figure \@ref(fig:censoring), the first and fifth observations suffers from a selection bias. 

```{r censoring, echo=FALSE, fig.cap="Censored and truncated data", out.height="250pt", out.width="500pt"}
knitr::include_graphics(path = "./imgs/censoring_and_truncation.png")
```

## Probabilistic concepts 

In survival analysis, the response variable denoted $T$ is a time-to-event variable. Instead of estimating the expected failure time, survival models estimate the *survival* and *hazard rate* functions which depend on the realization of $T$. 


### Survival function

The survival function $S(t)$ represents the probability that the considered event occurs after time $t$. For instance, $S(t)$ can measure the probability that a given customer survive in the portfolio at least until time $t$. Mathematically, the survival function is defined as: 
\begin{equation}
  S(t) = P(T > t) = 1 - F(t)
  (\#eq:survfun)
\end{equation}

where $F(t)$ is the cumulative distribution function. 

```{r survfunexp}
x <- seq(0, 10, .01)
theta <- 1
surv_exp <- 1 - pexp(q = x, rate = theta)
surv_fun_plot <- data.frame(x, surv_exp) %>%
  ggplot(aes(x = x, y = surv_exp)) +
  geom_line(size = 1, color = plot_col) +
  xlab(TeX("$t$")) + ylab(TeX("$S(t)$")) +
  theme(axis.title = element_text(size = 12), 
        axis.text = element_text(size = 11))
ggsave(filename = "./imgs/surv_fun_plot.png", 
       plot = surv_fun_plot) 
```

```{r survfunplot, echo=FALSE, fig.cap="Survival function $S_T(t)$ with $T \\sim \\mathcal{E} (1)$", out.height="300pt", out.width="400pt"}
knitr::include_graphics(path = "./imgs/surv_fun_plot.png")
```


### Hazard and Cumulative Hazard functions 

Another key concept in duration analysis is the hazard function $\lambda(t)$ which approximates the probability that the event occurs at time $t$. For instance, $\lambda(t)$ can measure the probability that a given individual leaves the firm portfolio at time $t$. Formally, it is expressed as follows: 
\begin{equation}
  \lambda(t) = \lim_{\Delta t \to 0} \frac{P\big[t \leq T < t + \Delta t | T \geq t \big]}{\Delta t}
  (\#eq:hazfun)
\end{equation}

Using the Bayes formula, equation \@ref(eq:hazfun) can also be written as (see equation \@ref(eq:hazfunproof) in appendix):
\begin{equation}
  \lambda(t) = \frac{-\text{d} \ln \big(S(t)\big)}{\text{d} t}
  (\#eq:hazfunbis)
\end{equation}

Finally, integrating the instantaneous hazard function gives the cumulative hazard function which can be more precisely estimated than the hazard function [@CAMERON_TRIVEDI] and is defined as: 

\begin{equation}
  \Lambda (t) = \int_{0}^{t} \lambda(s) \text{d}s = \ln \big(S(t)\big)
  (\#eq:cumhazfun)
\end{equation}

```{r}
cum_haz <- x 
cum_haz_plot <- data.frame(x, cum_haz) %>%
  ggplot(aes(x = x, y = cum_haz)) +
  geom_line(size = 1, color = plot_col) +
  xlab(TeX("$t$")) + ylab(TeX("$\\Lambda(t)$")) +
  theme(axis.title = element_text(size = 12), 
        axis.text = element_text(size = 11))
ggsave(filename = "./imgs/cum_haz_plot.png", 
       plot = cum_haz_plot) 
```

```{r cumhazplot, echo=FALSE, fig.cap="Cumulative Hazard function $\\Lambda_T(t)$ with $T \\sim \\mathcal{E} (1)$", out.height="300pt", out.width="400pt"}
knitr::include_graphics(path = "./imgs/cum_haz_plot.png")
```

Thus, the hazard, survival and cumulative hazard functions are three mathematical functions which describe the same distribution. 

## Nonparametric models {#nonparam}

When dealing with duration data, these methods are helpful to have a general overview of the raw (unconditional) hazard. Nonparametric models are rather used for data description rather than prediction. No explanatory variable is included in these models except for treatment variables such as the type of contract a customer has subscribe.  

### Notations

Let us consider a sample with $N$ observations with $k$ discrete failure time (e.g. a failure can be a churn event), such that $\forall j \in [\![1; k]\!]$ : 

- $t_j$ the $j^{\text{th}}$ discrete failure time, 
- $d_j$ the number of spells terminating at $t_j$,
- $m_j$ the number of right-censored spells in the interval $[t_j, t_{j+1}]$,
- $r_j$ the number of exposed durations right before time $t_j$ such that: 
\begin{equation}
  r_j = (d_j + m_j) + \dots + (d_k + m_k) = \sum_{l|l \geq j} (d_l + m_l)
  (\#eq:exposed)
\end{equation}

### Hazard function estimator

As the instantaneous hazard at time $t_j$ is defined as $\lambda_j = P[T=t_j|T\geq t_j]$, a trivial estimator of $\lambda_j$ is obtained by dividing the number of durations for which the event is realized in $t_j$ by the total number of exposed durations at time $t_j^{-}$. Formally, it is expressed as: 

\begin{equation}
  \hat{\lambda}_j = \frac{d_j}{r_j}
  (\#eq:hazest)
\end{equation}

### Kaplan-Meier estimator

Once the hazard function estimator computed, the discrete-time survivor function can be estimated using the Kaplan-Meier product-limit estimator. To estimate the survival at time $t$, this estimator computes the joint probability that a spell stays in the same state until $t$ (e.g. remaining loyal to a firm until a certain time). This method is based on conditional probabilities and the survival function estimate is defined as: 

\begin{equation}
  \hat{S}(t) = \Pi_{j|t_j \leq t} \big(1-\hat{\lambda}_j\big) = \Pi_{j|t_j \leq t}\frac{r_j - d_j}{r_j}
  (\#eq:kaplanmeier)
\end{equation}

When plotting the survival curve after having performed the Kaplan-Meier estimation, confidence bands are also added to the plot in order to reflect sampling variability [@CAMERON_TRIVEDI]. The confidence interval of the survival function $S(t)$ is derived from the estimate of the variance of $S(t)$ which is obtained by the Grenwood estimate (see equation \@ref(eq:greenwood)).

\begin{equation}
  \hat{\mathrm{V}}[\hat{S}(t)] = \hat{S}(t)^2 \sum_{j|t_j \leq t} \frac{d_j}{r_j(r_j-d_j)}
  (\#eq:greenwood)
\end{equation}

### Nelson-Aalen estimator

The cumulative hazard function estimate is given by the Nelson-Aalen estimator which consists in summing up the hazard estimates for each failure time. 

\begin{equation}
  \hat{\Lambda}(t) = \sum_{j | t_j \leq t} \hat{\lambda}_{j} = \sum_{j | t_j \leq t} \frac{d_j}{r_j}
  (\#eq:nelsonaalen)
\end{equation}

Exponentiating $\hat{\Lambda}(t)$, one can obtain a second estimate of the survival function (see equation \@ref(eq:linksurvcumhaz) in the appendix): 

\begin{equation}
    \tilde{S}(t) = \exp \big( -\hat{\Lambda}(t) \big)
    (\#eq:survest)
\end{equation}

## Parametric models

The nonparametric estimation is undoubtedly useful when it comes to have a general overview on the survival data. However, one may want to model the hazard and survivor functions with a functional form in which unknown parameters need to be optimized. 

When implementing parametric models, $\lambda$, $S$ and $\Lambda$ are expressed based on the chosen parametric form. The instantaneous hazard function can either be constant or monotone. In our study we assume that the explanatory variables are time-constant as we do not have dynamic data at our disposal. 

Parametric estimation has a twofold purpose that is to implement a robust model to estimate the risk that a specific event occurs while identifying the variables (or covariates) which best explain this risk. 

### Constant hazard (exponential model)

The exponential distribution models the time between events in a Poisson process and has the key property of being *memoryless*. Let us note $T$ a time-to-event variable following such that $T \sim \mathcal{E}(\theta)$ where $\theta$ is the *rate* parameter. In this context, *memorylessness* can be defined as follows: 

\begin{equation}
  \mathbb{P}(T>t+s | T > t) = \mathbb{P}(T>s)
  (\#eq:memorylessness)
\end{equation}

$\forall t \geq 0\ ,\ \theta > 0$ the density, hazard and survival functions can be expressed as:

\begin{equation}
  \begin{aligned}
    f_{\theta}(t) & = \theta e^{-\theta t} \\\\
    \lambda_{\theta}(t) & = \theta \\\\
    S_{\theta}(t) & = e^{-\theta t} \\\\
  \end{aligned}
  (\#eq:exponential)
\end{equation}

Thus, the exponential distribution is characterized by a **constant** hazard function which is a consequence of *memorylessness*. 

### Monotone hazard

#### Weibull model {-}

The Weibull distribution is a less restrictive generalization of the exponential distribution defined by a shape parameter $\nu$ and a scale parameter $\theta$. 

$\forall t \geq 0$ et $k=\nu,\ \theta > 0$ the density, hazard and survival functions can be expressed as:

\begin{equation}
  \begin{aligned}
      \lambda_{k,\theta}(t) & = \nu \bigg(\frac{1}{\theta}\bigg)^{\nu} t^{\nu - 1} \\\\
      S_{k, \nu}(t) & = \exp \Bigg( -\bigg(\frac{1}{\theta}\bigg)^{\nu} t\Bigg) 
  \end{aligned}
  (\#eq:weibull)
\end{equation}

The instantaneous hazard function $\lambda_{\nu,\theta}$ is:

- monotonic **decreasing** if $\nu \in [0, 1]$ (e.g. customer loyalty),
- **constant** if $\nu=1$ and $T \sim \mathcal{E}(\theta)$,
- monotonic **increasing** if $\nu > 1$ (e.g. customers leaving the portfolio to a competing firm).

Figure \@ref(fig:weibullplots) illustrate the hazard and survivor functions associated to a variable $T$ which follows a Weibull distribution. The two curves' shape depend both on the shape ($\nu$) and scale parameters ($\theta$). Some remarks can be made looking at the two plots. On the one hand, the higher $\nu$, the more increasing the hazard function. Furthermore, when $\nu < 1$ the hazard function is decreasing meaning that the risk of the event occurring decreases as time goes by. When $\nu > 1$ the hazard function is convex which indicates that a marginal increase in time leads to an increase (decrease) of over one unit in the the hazard (survivor) function. On the other hand, it can be noted that the Weibull distribution corresponds to the exponential distribution when $\nu = \theta = 1$ (see figures \@ref(fig:survfunplot) and \@ref(fig:cumhazplot)). 

```{r weibull, eval=FALSE}
shape <- c(.5, 1, 1.5, 3)
scale <- c(1, 2, 3, 4)
t <- seq(0, 10, .01)

haz_wei <- function(t, shape, scale){
  return( shape*(1/scale)**shape * t**(shape-1) ) 
}
surv_wei <- function(q, shape, scale){
  return( 1- pweibull(q=q, shape=shape, scale=scale) ) 
}

make_data_frame <- function(type){
  
  # initialize matrix 
  init <-  matrix(
    nrow = length(t), 
    ncol = length(shape)*length(scale)
  )
  
  # name the matrix columns with the combinations (shape, scale)
  make_colnames <- function(){
    lapply(
      shape, 
      function(sh){
        lapply(
          scale, 
          function(sc){ paste("(", sh, ", ", sc, ")", sep = "") }
        )
      }
    ) %>% unlist()
  }
  
  colnames(init) <- make_colnames()
  
  # populate the matrix with either hazard or survival function values
  for (sh in shape){
    for (sc in scale){
      col <- paste("(", sh, ", ", sc, ")", sep = "")
      if (type == "haz"){ 
        init[, col] <- haz_wei(t = t, shape = sh, scale = sc) 
      } 
      else {
        init[, col] <- surv_wei(q = t, shape = sh, scale = sc)
      }
    }
  }
  
  # convert to dataframe and select interested columns
  init %>% 
    as.data.frame() %>%
    select(c(
      "(0.5, 2)", "(1, 1)", 
      "(1, 2)", "(1.5, 3)", 
      "(3, 4)" 
    )) %>%
    mutate(t = t) %>%
    pivot_longer(cols = 1:5)
  
}

haz <- make_data_frame(type = "haz")
surv <- make_data_frame(type = "surv") 
```

```{r weibull plots, eval=FALSE}
make_plot <- function(type){
  
  if (type == "haz"){ 
    data <- haz 
    ylab <- TeX("$\\lambda(t)$")
  } 
  else {
    data <- surv 
    ylab <- TeX("$S(t)$")
  }
  
  data %>%
    ggplot( aes(x = t, y = value, color = name) ) +
    geom_line(size = .8) +
    scale_color_brewer(palette = "Set2") +
    labs( x = TeX("$t$"), y = ylab ) +
    guides( color = guide_legend( 
      title = TeX("$(\\nu, \\theta)$"), 
      title.position = "left", 
      title.hjust = 1) ) +
    theme( legend.position = "bottom", 
           legend.title = element_text(size = 12), 
           legend.text = element_text(size = 12), 
           axis.title = element_text(size = 12), 
           axis.text = element_text(size = 11) ) 
  
}

weibull_plots <- ggarrange(
  make_plot(type = "haz"), 
  make_plot(type = "surv"), 
  ncol = 2, 
  common.legend = T, 
  legend = "bottom"
)

ggsave(filename = "./imgs/weibull_plots.png", 
       plot = weibull_plots, 
       width = 10, height = 5)
``` 


```{r weibullplots, echo=FALSE, fig.cap="Hazard and Survival functions with $T \\sim \\mathcal{W} (\\nu, \\theta)$", out.height="350pt", out.width="700pt"}
knitr::include_graphics(path = "./imgs/weibull_plots.png")
```

#### Other models {-}

Different probabilistic distributions can be chosen the model the hazard and survival functions related to a time-to-event variable. The Gompertz model is usually used for mortality data in biostatistics. The gamma model depends both on the gamma and inverse-gamma distribution and is also based on shape and scale parameters. 

### Concave and convex hazard

When the hazard related to the studied phenomenon varies, the distributions introduced above are limited as they model monotone hazard. The generalized Weibull model appears to be a good choice to estimate phenomena with concave or convex hazard. It is based on three parameters: $\nu$ (shape), $\theta$ (scale) and $\gamma$. When $\gamma = 1$, the generalized Weibull becomes the Weibull distribution $\mathcal{W}(\nu, \theta)$. 

```{r eval=FALSE}
### Parametric estimation with covariates: AFT models

#*Accelerated failure time* (AFT) models specify that a set of covariates has a multiplicative impact on the failure time or an additive impact on the log failure time. In other words, the covariate's effect is to increase or decrease the duration of a spell. Modelling $\ln t$ rather than $t$, the AFT model is specified as follows: 

#\begin{equation}
#\ln t = \pmb{\mathrm{x}}^{'}\beta + u
#(\#eq:aft)
#\end{equation}

#where $u$ is a random variable following a specific distribution on $(-\infty, \infty)$ which can be: 

#- log-normal, 
#- log-logistic, 
#- exponential, 
#- Weibull, 
#- gamma. 

#In this context, the instantaneous hazard function can be expressed as follows #(see equation \@ref(eq:hazaftproof) in the appendix): 

#\begin{equation}
#\lambda(t|\pmb{\mathrm{x}}) = \lambda_0 \big(t \text{e}^{-\pmb{\mathrm{x}'}\beta} \big)  \text{e}^{\pmb{\mathrm{x}'}\beta}
#(\#eq:hazaft)
#\end{equation}

#where $\lambda_0(.)$ denotes the baseline hazard function. 

#When $\text{e}^{-\pmb{\mathrm{x}'}\beta} > 1$ the covariates accelerate the baseline hazard whereas the latter is decelerated when $\text{e}^{-\pmb{\mathrm{x}'}\beta} < 1$. It can also be noted that in the case of *accelerated failure time* models, for a covariate $x_k > 0$, a negative regression coefficient leads to a smaller survival time. 

```



## Semi-parametric estimation 

### Proportional Hazards models

Parametric models assume that the baseline (or raw) hazard belongs to a specific distribution. This assumption can be sometimes too restrictive and semi-parametric models can be more adapted to describe the duration data. 

In *proportional hazards* (PH) models, the instantaneous risk function is \textbf{proportional} to the baseline hazard $\lambda_0 (t,\alpha)$ modulo a \textbf{scaling factor} depending on the covariates $\phi(\pmb{\mathrm{x}}, \beta)$. These models allow to generalize the basic survival models to a survival regression model which permits to take individuals' heterogeneity into consideration [@RMS].  The general mathematical formulation is expressed as follows: 

\begin{equation}
    \lambda(t|\pmb{\mathrm{x}}) = \lambda_0 (t,\alpha)  \phi(\pmb{\mathrm{x}}, \beta) 
    (\#eq:ph)
\end{equation}

Note that when the function form of $\lambda_0 (t,\alpha)$ is known, we are in the case of parametric estimation. For instance, the exponential, Weibull and Gompertz models are PH models since their respective hazards are function of some covariates. 

#### What does *proportional hazards* mean? {-}

PH models are said to be proportional as the relative hazard ratio between two individuals $i$ and $k$ does not vary over time, such that: 

\begin{equation}
  \frac{\lambda(t|\pmb{\mathrm{x}_i})}{\lambda(t|\pmb{\mathrm{x}_k})} = \frac{\phi(\pmb{\mathrm{x}_i}, \beta) }{\phi(\pmb{\mathrm{x}_k}, \beta)}
  (\#eq:prophazratio)
\end{equation}

The formulation stated in equation \@ref(eq:prophazratio) needs to be verified when one wants to fit a PH model to real-life data.

#### Marginal effects {-}

In *proportional hazards* models, the marginal effect of covariate $x_p$ on the hazard function can be easily derived since this computation only requires knowledge on $\beta$. As shown in @CAMERON_TRIVEDI, a one-unit increase in the $p^{\text{th}}$ covariate leads to the following variation in the hazard function *ceteris paribus*: 

\begin{equation}
    \frac{\partial \lambda(t|\pmb{\mathrm{x}}, \beta)}{\partial x_p} = \lambda(t|\pmb{\mathrm{x}}, \beta) \frac{\partial \phi(\pmb{\mathrm{x}}, \beta) / \partial x_p}{\phi(\pmb{\mathrm{x}}, \beta) }
    (\#eq:meph)
\end{equation}

#### Partial likelihood estimation {-}

The vector of parameters $\beta$ related to the regression part of the PH model is estimated by partial likelihood maximization. The method's principle consists in only estimating the regression's parameters $\beta$ by considering the baseline hazard $\lambda_0$ as noise. If desired an estimate of the baseline hazard can be recovered after estimation of $\beta$ using for instance, the Nelson-Aalen estimator (see \@ref(nonparam)). Cox's intuition is that no information can be retrieved from the intervals during which no event has occurred and that it is conceivable that $\lambda_0$ is null in these intervals. Thus, solely the set of moments when an event occurs are considered in the estimation method. 

In order to derive the partial likelihood function, let us note $t_j$ the $j^{\text{th}}$ discrete failure time in an $N$-sample with $j \in [\![1; k]\!]$, such that:

- $t_1 < t_2 < \dots < t_k$, 
- $D(t_j) = \{l: t_l = t_j\}$ is the set of spells completed at $t_j$ with $\#D(t_j) = d_j$, 
- $R(t_j) = \{l: t_l \geq t_i\}$ is the set of spells at risk at $t_j$.

The contribution of a spell in $D(t_j)$ to the likelihood function equals the conditional probability that the spell ends at $t_j$ given it is exposed at that specific time and can be written as (see @CAMERON_TRIVEDI and proof \@ref(eq:contribpartiallikproof) for more details): 

\begin{equation}
  \mathbb{P}\big[T_j = t_j | R(t_j) \big] = \frac{\phi(\pmb{\mathrm{x}_j}, \beta)}{\sum_{l \in R(t_j)} \phi(\pmb{\mathrm{x}_l}, \beta)}
  (\#eq:contribpartiallik)
\end{equation}

Given $k$ discrete failure times are considered and that for each of those there is a set $D(t_j)$ of completed spells, Cox defines the partial likelihood function as the joint product of the probability expressed in \@ref(eq:contribpartiallik), such that:

\begin{equation}
  \mathcal{L}_p = \Pi_{j=1}^{k} \ \frac{\Pi_{m \in D(tj)} \ \phi(\pmb{\mathrm{x}_j}, \beta)}{\Big[\sum_{l \in R(t_j)} \phi(\pmb{\mathrm{x}_l}, \beta)\Big]^{d_j}}
  (\#eq:partlik)
\end{equation}

The latter formulation of the partial likelihood function is explained in more details in equations \@ref(eq:partlikproof) and \@ref(eq:partlikproofbis) in the appendix. 

### Cox PH model

The Cox *proportional hazards* model is the most popular for the analysis of duration data. This model is said to be semi-parametric as it makes no assumption regarding the nature of the baseline hazard function $\lambda_0(t)$. The parametric part only relies in the modelling of the effect of some covariates on the hazard function $\lambda(t)$. The relationship between the vector of covariates and the log hazard is linear and the parameters can be estimated by maximizing the partial likelihood function. The Cox PH model solely assumes that predictors act multiplicatively on the hazard function. The model is formulated as in equation \@ref(eq:ph) with the exponential function as link function between the hazard and the covariates i.e. $\lambda(t|\pmb{\mathrm{x}}) = \lambda_0 (t,\alpha)  \text{e}^{\pmb{\mathrm{x'}} \beta}$. 




