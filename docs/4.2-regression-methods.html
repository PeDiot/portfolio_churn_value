<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4.2 Regression methods | Portfolio, Churn &amp; Customer Value</title>
  <meta name="description" content="This research paper aims at modelling customer portfolio, churn and customer value." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="4.2 Regression methods | Portfolio, Churn &amp; Customer Value" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This research paper aims at modelling customer portfolio, churn and customer value." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4.2 Regression methods | Portfolio, Churn &amp; Customer Value" />
  
  <meta name="twitter:description" content="This research paper aims at modelling customer portfolio, churn and customer value." />
  

<meta name="author" content="Hugo Cornet, Pierre-Emmanuel Diot, Guillaume Le Halper, Djawed Mancer" />


<meta name="date" content="2021-12-23" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="4.1-machine-learning-for-survival-data.html"/>
<link rel="next" href="4.3-performance-metrics.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Portfolio, Churn & Customer Value</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="1" data-path="1-intro.html"><a href="1-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="1.1-portfoliodef.html"><a href="1.1-portfoliodef.html"><i class="fa fa-check"></i><b>1.1</b> How to define a customer portfolio?</a></li>
<li class="chapter" data-level="1.2" data-path="1.2-attritiondef.html"><a href="1.2-attritiondef.html"><i class="fa fa-check"></i><b>1.2</b> What is attrition?</a></li>
<li class="chapter" data-level="1.3" data-path="1.3-valuedef.html"><a href="1.3-valuedef.html"><i class="fa fa-check"></i><b>1.3</b> What does customer “value” mean?</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-literature.html"><a href="2-literature.html"><i class="fa fa-check"></i><b>2</b> Literature Review</a>
<ul>
<li class="chapter" data-level="2.1" data-path="2.1-portfolio.html"><a href="2.1-portfolio.html"><i class="fa fa-check"></i><b>2.1</b> On customer portfolio</a></li>
<li class="chapter" data-level="2.2" data-path="2.2-attrition.html"><a href="2.2-attrition.html"><i class="fa fa-check"></i><b>2.2</b> On attrition</a></li>
<li class="chapter" data-level="2.3" data-path="2.3-value.html"><a href="2.3-value.html"><i class="fa fa-check"></i><b>2.3</b> On customer value</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-duration.html"><a href="3-duration.html"><i class="fa fa-check"></i><b>3</b> Duration models</a>
<ul>
<li class="chapter" data-level="3.1" data-path="3.1-definition.html"><a href="3.1-definition.html"><i class="fa fa-check"></i><b>3.1</b> Definition</a></li>
<li class="chapter" data-level="3.2" data-path="3.2-censoring-and-truncation.html"><a href="3.2-censoring-and-truncation.html"><i class="fa fa-check"></i><b>3.2</b> Censoring and Truncation</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="3.2-censoring-and-truncation.html"><a href="3.2-censoring-and-truncation.html#censoring-mechanisms"><i class="fa fa-check"></i><b>3.2.1</b> Censoring mechanisms</a></li>
<li class="chapter" data-level="3.2.2" data-path="3.2-censoring-and-truncation.html"><a href="3.2-censoring-and-truncation.html#selection-bias"><i class="fa fa-check"></i><b>3.2.2</b> Selection bias</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3.3-probabilistic-concepts.html"><a href="3.3-probabilistic-concepts.html"><i class="fa fa-check"></i><b>3.3</b> Probabilistic concepts</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="3.3-probabilistic-concepts.html"><a href="3.3-probabilistic-concepts.html#survival-function"><i class="fa fa-check"></i><b>3.3.1</b> Survival function</a></li>
<li class="chapter" data-level="3.3.2" data-path="3.3-probabilistic-concepts.html"><a href="3.3-probabilistic-concepts.html#hazard-and-cumulative-hazard-functions"><i class="fa fa-check"></i><b>3.3.2</b> Hazard and Cumulative Hazard functions</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3.4-nonparam.html"><a href="3.4-nonparam.html"><i class="fa fa-check"></i><b>3.4</b> Nonparametric models</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="3.4-nonparam.html"><a href="3.4-nonparam.html#notations"><i class="fa fa-check"></i><b>3.4.1</b> Notations</a></li>
<li class="chapter" data-level="3.4.2" data-path="3.4-nonparam.html"><a href="3.4-nonparam.html#hazard-function-estimator"><i class="fa fa-check"></i><b>3.4.2</b> Hazard function estimator</a></li>
<li class="chapter" data-level="3.4.3" data-path="3.4-nonparam.html"><a href="3.4-nonparam.html#kaplan-meier-estimator"><i class="fa fa-check"></i><b>3.4.3</b> Kaplan-Meier estimator</a></li>
<li class="chapter" data-level="3.4.4" data-path="3.4-nonparam.html"><a href="3.4-nonparam.html#nelson-aalen-estimator"><i class="fa fa-check"></i><b>3.4.4</b> Nelson-Aalen estimator</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3.5-parametric-models.html"><a href="3.5-parametric-models.html"><i class="fa fa-check"></i><b>3.5</b> Parametric models</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="3.5-parametric-models.html"><a href="3.5-parametric-models.html#constant-hazard-exponential-model"><i class="fa fa-check"></i><b>3.5.1</b> Constant hazard (exponential model)</a></li>
<li class="chapter" data-level="3.5.2" data-path="3.5-parametric-models.html"><a href="3.5-parametric-models.html#monotone-hazard"><i class="fa fa-check"></i><b>3.5.2</b> Monotone hazard</a></li>
<li class="chapter" data-level="3.5.3" data-path="3.5-parametric-models.html"><a href="3.5-parametric-models.html#concave-and-convex-hazard"><i class="fa fa-check"></i><b>3.5.3</b> Concave and convex hazard</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="3.6-semi-parametric-estimation.html"><a href="3.6-semi-parametric-estimation.html"><i class="fa fa-check"></i><b>3.6</b> Semi-parametric estimation</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="3.6-semi-parametric-estimation.html"><a href="3.6-semi-parametric-estimation.html#proportional-hazards-models"><i class="fa fa-check"></i><b>3.6.1</b> Proportional Hazards models</a></li>
<li class="chapter" data-level="3.6.2" data-path="3.6-semi-parametric-estimation.html"><a href="3.6-semi-parametric-estimation.html#cox-ph-model"><i class="fa fa-check"></i><b>3.6.2</b> Cox PH model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-ml.html"><a href="4-ml.html"><i class="fa fa-check"></i><b>4</b> Machine Learning</a>
<ul>
<li class="chapter" data-level="4.1" data-path="4.1-machine-learning-for-survival-data.html"><a href="4.1-machine-learning-for-survival-data.html"><i class="fa fa-check"></i><b>4.1</b> Machine Learning for Survival Data</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="4.1-machine-learning-for-survival-data.html"><a href="4.1-machine-learning-for-survival-data.html#survival-trees"><i class="fa fa-check"></i><b>4.1.1</b> Survival Trees</a></li>
<li class="chapter" data-level="4.1.2" data-path="4.1-machine-learning-for-survival-data.html"><a href="4.1-machine-learning-for-survival-data.html#random-survival-forests-rsf"><i class="fa fa-check"></i><b>4.1.2</b> Random Survival Forests (RSF)</a></li>
<li class="chapter" data-level="4.1.3" data-path="4.1-machine-learning-for-survival-data.html"><a href="4.1-machine-learning-for-survival-data.html#cox-boosting"><i class="fa fa-check"></i><b>4.1.3</b> Cox Boosting</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4.2-regression-methods.html"><a href="4.2-regression-methods.html"><i class="fa fa-check"></i><b>4.2</b> Regression methods</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="4.2-regression-methods.html"><a href="4.2-regression-methods.html#linear-models"><i class="fa fa-check"></i><b>4.2.1</b> Linear models</a></li>
<li class="chapter" data-level="4.2.2" data-path="4.2-regression-methods.html"><a href="4.2-regression-methods.html#more-advanced-regression-models"><i class="fa fa-check"></i><b>4.2.2</b> More advanced regression models</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4.3-performance-metrics.html"><a href="4.3-performance-metrics.html"><i class="fa fa-check"></i><b>4.3</b> Performance Metrics</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="4.3-performance-metrics.html"><a href="4.3-performance-metrics.html#metrics-for-regression-models"><i class="fa fa-check"></i><b>4.3.1</b> Metrics for Regression models</a></li>
<li class="chapter" data-level="4.3.2" data-path="4.3-performance-metrics.html"><a href="4.3-performance-metrics.html#metrics-for-survival-models"><i class="fa fa-check"></i><b>4.3.2</b> Metrics for Survival models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-data.html"><a href="5-data.html"><i class="fa fa-check"></i><b>5</b> Data</a>
<ul>
<li class="chapter" data-level="5.1" data-path="5.1-general-overview.html"><a href="5.1-general-overview.html"><i class="fa fa-check"></i><b>5.1</b> General Overview</a></li>
<li class="chapter" data-level="5.2" data-path="5.2-target-variables.html"><a href="5.2-target-variables.html"><i class="fa fa-check"></i><b>5.2</b> Target Variables</a>
<ul>
<li><a href="5.2-target-variables.html#churn_value-and-tenure_months"><code>Churn_Value</code> and <code>Tenure_Months</code></a></li>
<li class="chapter" data-level="5.2.1" data-path="5.2-target-variables.html"><a href="5.2-target-variables.html#cltv"><i class="fa fa-check"></i><b>5.2.1</b> <code>CLTV</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a>
<ul>
<li class="chapter" data-level="" data-path="hazard-function.html"><a href="hazard-function.html"><i class="fa fa-check"></i>Hazard function</a></li>
<li class="chapter" data-level="" data-path="link-between-cumulative-hazard-and-survivor-functions.html"><a href="link-between-cumulative-hazard-and-survivor-functions.html"><i class="fa fa-check"></i>Link between cumulative hazard and survivor functions</a></li>
<li class="chapter" data-level="" data-path="contribution-to-the-partial-likelihood-function-in-ph-models.html"><a href="contribution-to-the-partial-likelihood-function-in-ph-models.html"><i class="fa fa-check"></i>Contribution to the partial likelihood function in PH models</a></li>
<li class="chapter" data-level="" data-path="partial-likelihood-function-in-ph-models.html"><a href="partial-likelihood-function-in-ph-models.html"><i class="fa fa-check"></i>Partial likelihood function in PH models</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Portfolio, Churn &amp; Customer Value</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regression-methods" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> Regression methods</h2>
<p>Customer Lifetime Value being a quantitative variable, machine learning regression models are adapted to predict this quantity. Regression analysis is a fundamental concept in the field of machine learning. It falls under supervised learning wherein the algorithm is trained with both input features and output labels. It helps in establishing a relationship among the variables by estimating how one variable affects the other. In this context, we present some famous machine learning algorithms that can be implemented to estimate the relationship between several features and a continuous target variable.</p>
<div id="linear-models" class="section level3" number="4.2.1">
<h3><span class="header-section-number">4.2.1</span> Linear models</h3>
<div id="model-formulation" class="section level4 unnumbered">
<h4>Model formulation</h4>
<p>Linear models are called that way as the target value is expected to be a linear combination of the features. Let <span class="math inline">\(\hat{\mathrm{y}}\)</span> be the vector of predicted values, <span class="math inline">\(\beta\)</span> the set of parameters to optimise and <span class="math inline">\(\pmb{\mathrm{x}}\)</span> the feature vector with <span class="math inline">\(\pmb{\mathrm{x}} = [\pmb{1}, \mathrm{x}_1, \dots, \mathrm{x}_P]\)</span>. The linear relationship between <span class="math inline">\(\hat{\mathrm{y}}\)</span> and <span class="math inline">\(\pmb{\mathrm{x}}\)</span> can be written as follows:</p>
<p><span class="math display" id="eq:linearmodel">\[\begin{equation}
  \hat{\mathrm{y}} = \beta_0 + \beta_1 \mathrm{x}_1 + \dots + \beta_P \mathrm{x}_P
  \tag{4.2}
\end{equation}\]</span></p>
<p>with <span class="math inline">\(P\)</span> the number of explanatory variables.</p>
<p>Equation <a href="4.2-regression-methods.html#eq:linearmodel">(4.2)</a> can be rewritten in a vectorized way such that <span class="math inline">\(\hat{\mathrm{y}} = \pmb{\mathrm{x}}\beta\)</span>.</p>
</div>
<div id="ordinary-least-squares" class="section level4 unnumbered">
<h4>Ordinary Least Squares</h4>
<p>The most standard regression algorithm is linear regression in which the loss function is the residual sum of squares between the observed targets in the dataset <span class="math inline">\(\mathrm{y}\)</span> and the targets predicted by the linear formulation <span class="math inline">\(\hat{\mathrm{y}}\)</span>. This method is called ordinary least squares (OLS). Mathematically, the set of parameters <span class="math inline">\(\beta\)</span> is chosen with a view of minimizing:
<span class="math display" id="eq:ols">\[\begin{equation}
  \mathcal{l}_{OLS} = ||\mathrm{y} - \pmb{\mathrm{x}}\beta||_2 = (\mathrm{y} - \pmb{\mathrm{x}}\beta)^{&#39;}(\mathrm{y} - \pmb{\mathrm{x}}\beta)
  \tag{4.3}
\end{equation}\]</span></p>
</div>
<div id="regularization" class="section level4 unnumbered">
<h4>Regularization</h4>
<p>In order to address some of the problems encountered with the OLS method, regularization can be used. Regularization techniques can be employed when the explanatory variables are highly correlated.</p>
<p>Ridge regression is a linear regression with a quadratic constraint on the coefficients. Here, the coefficients minimize a penalized residual sum of squares: the higher the penalty term, the more large coefficients are discouraged and the less risk of overfitting. Formally, the ridge loss function is based on <span class="math inline">\(l_2\)</span>-norm and is expressed as:</p>
<p><span class="math display" id="eq:ridge">\[\begin{equation}
  \mathcal{l}_{Ridge} = ||\mathrm{y} - \pmb{\mathrm{x}}\beta||_2 + \alpha ||\beta||_2
  \tag{4.4}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\alpha\)</span> is the shrinkage parameter which controls the penalization on the value of the model’s coefficients.</p>
<p>Lasso regression is another example of penalized estimation technique with this time a linear constraint on the <span class="math inline">\(\beta\)</span> vector. It is helpful in reducing the number of features upon which the target variable is dependent. The loss function is based on <span class="math inline">\(l_1\)</span>-norm and can be written as follows:</p>
<p><span class="math display" id="eq:lasso">\[\begin{equation}
  \mathcal{l}_{Lasso} = ||\mathrm{y} - \pmb{\mathrm{x}}\beta||_2 + \alpha ||\beta||_1
  \tag{4.5}
\end{equation}\]</span></p>
<p>Finally, ElasticNet is a linear regression model trained with both <span class="math inline">\(l_1\)</span> and <span class="math inline">\(l_2\)</span>-norm regularization of the coefficients and is useful when there are multiple features that are correlated with one another. In this context, the loss function is derived as follows:</p>
<p><span class="math display" id="eq:elasticnet">\[\begin{equation}
  \mathcal{l}_{EN} = ||\mathrm{y} - \pmb{\mathrm{x}}\beta||_2 + \alpha \rho ||\beta||_1 + \alpha \frac{1-\rho}{2} ||\beta||_2
  \tag{4.6}
\end{equation}\]</span></p>
</div>
<div id="gradient-descent" class="section level4 unnumbered">
<h4>Gradient Descent</h4>
<p>The loss functions presented above need to be optimized to obtain the optimal set of parameters that best represents the linear relationship between <span class="math inline">\(\mathrm{y}\)</span> and <span class="math inline">\(\pmb{\mathrm{x}}\)</span>. In other words, a minimization problem needs to be solved. Gradient descent is an algorithm whose goal is to find the maximum (or minimum) of a given function <span class="math inline">\(f\)</span>. The gradient of <span class="math inline">\(f\)</span> is defined as the vector of partial derivatives and gives the input direction in which <span class="math inline">\(f\)</span> most quickly increases. The gradient descent approach consists in picking a random starting point, computing the gradient, taking a small step in the opposite direction of the gradient and repeating with the new starting point until some criterion is met.</p>
</div>
</div>
<div id="more-advanced-regression-models" class="section level3" number="4.2.2">
<h3><span class="header-section-number">4.2.2</span> More advanced regression models</h3>
<p>Linear models are considered the most common and easy-to-understand models when it comes to predict a quantitative variable such as customer <em>value.</em> Nonetheless, more advanced techniques can sometimes be implemented in order to obtain more accurate predictions.</p>
<div id="generalized-additive-model-gam" class="section level4 unnumbered">
<h4>Generalized Additive Model (GAM)</h4>
<p>GAM is a statistical model in which the response variable <span class="math inline">\(\mathrm{y}\)</span> depends linearly on unknown smooth functions of some feature vector <span class="math inline">\(\pmb{\mathrm{x}}\)</span>, and interest focuses on inference about these smooth functions called <span class="math inline">\(f_p\)</span>. The <span class="math inline">\(f_p\)</span> functions may be either parametric (polynomial), semi-parametric or nonparametric (smoothing splines) leading to more flexibility in the model’s assumptions on the actual relationship between <span class="math inline">\(\mathrm{y}\)</span> and <span class="math inline">\(\pmb{\mathrm{x}}\)</span>. A link function <span class="math inline">\(g\)</span> can also be introduced to specify this relationship and the model’s general formulation is as follows:</p>
<p><span class="math display" id="eq:gam">\[\begin{equation}
  g\big(\mathrm{E} [\mathrm{y}] \big) = \sum_{p=1}^P \ f_p(\mathrm{x}_p)
  \tag{4.7}
\end{equation}\]</span></p>
</div>
<div id="other-models-1" class="section level4 unnumbered">
<h4>Other models</h4>
<p>State-of-the-art algorithms can also be employed to describe the relationship between the continuous variable and the feature vector. Among them, one can find random forest of regression trees, gradient boosting as well as multi-layer perceptron(MLP) regressor.</p>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="4.1-machine-learning-for-survival-data.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="4.3-performance-metrics.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["portfolio_churn_value.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
