[["index.html", "Portfolio, Churn &amp; Customer Value Abstract", " Portfolio, Churn &amp; Customer Value Hugo Cornet, Pierre-Emmanuel Diot, Guillaume Le Halper, Djawed Mancer 2021-12-17 Abstract This paper is being realized as part of our last year in masters degree in economics. It aims at studying the firms most valuable asset namely its customers. To that end, we adopt a quantitative approach based on econometrics and data analysis with a threefold purpose to : model customer portfolio as a set of customer segments; predict and analyse customer churn; estimate customer portfolios overall value. After having defined the subjects key concepts, we apply duration models and machine learning techniques to a kaggle dataset related to customers of a fictional telecommunications service provider (TSP). Keywords: customer portfolio management (CPM), churn, customer value, duration models, machine learning, telecom. "],["1-intro.html", "Chapter 1 Introduction", " Chapter 1 Introduction In a world in which the access to information is almost free or insignificant, and where there is a real plurality of offers, churn analysis has become one of the key points a firm needs to focus on. Whoever says plurality of offers needs to introduce the term competition. Thereby, the latter is more and more fierce and cutthroat. Furthermore, thanks to some laws, switching costs have decreased significantly. For instance in France, when you switch your internet provider, the new provider pays you off cancellation fees. All of this being said, we shall recall the development, if not the advent of both survival analysis and machine learning. Latters have enabled companies to really push-up their strategies in terms of customer management and churn analysis. Such a draft is full of incentives. Tons of questions were asked. Sorting through them is no small feet. After much thought, only a few questions have managed to make out alright. How to model the full value of a customer portfolio? How to partition a client portfolio? Chich criteria can be relied on? Which tools to use? How to predict the lifespan of a customer / customer segment? How to predict churn? How to prevent it? In order to address them, some objectives have been established: Estimate the remaining operational life of a client / client segment Model portfolio value (return, risk) with econometrical techniques Predict churn with machine learning methods for classification Fraction groups of customers according to their value in the portfolio Those ones shall be met by the use of advanced techniques of data analysis. "],["1.1-portfoliodef.html", "1.1 How to define a customer portfolio?", " 1.1 How to define a customer portfolio? The notion of portfolio has greatly evolved before a firms consumer base was considered as a set of portfolios. In chapter 2 a part of the literature review depicts an evolution of the portfolio management notion. A customer portfolio can be defined as a set of customers divided into several segments (or clusters) based on similar attributes. These discriminant features can be both economic - willingness to pay, budget constraint, etc. - and sociological - gender, age, socio-professional category, etc. . The underlying objective of this segmentation is to optimally allocate the companys resources. When dealing with customer portfolio management (CPM), two dimensions can be considered. On the one hand, it can be assumed that a customer stays in the same segment throughout their life in the firms portfolio. On the other hand, a dynamic approach can be adopted as suggested by Homburg on dynamics in customer portfolios (Homburg, Steiner, and Totzek 2009). In their article, the authors question the static analysis by assuming that a customer can switch between segments. They explain that one of the firms objectives is to convert less valuable customers into more valuable ones. In this context, customers can also be segmented depending on their value to the firm and their cost to serve (Thakur and Workmanb 2016), as depicted by figure 1.1. Figure 1.1: CPM Matrix References "],["1.2-attritiondef.html", "1.2 What is attrition?", " 1.2 What is attrition? Customer attrition or churn occurs when a client discontinues using a service or a product offered by a firm. Churn analysis corresponds to measuring the attrition rate in the customer base of any company. However, evaluating attrition depends on the type of relationship between the firm and its clients. When it is defined by a contract, the customer has to inform the firm about their service termination. In the telecom industry, a consumer is required to notify their telecommunications service provider (TSP) before going to a competing company. In an opposite way, the firm/client relationship can be non-contractual. In that case, the service termination does not need to be notified. Attrition then becomes a latent variable and more advanced models are used to make forecasts. "],["1.3-valuedef.html", "1.3 What does customer value mean?", " 1.3 What does customer value mean? In customer portfolio management, one clients value is represented by the customer lifetime value (CLV). CLV is the present value of all future purchases made by a customer over their lifetime in the firms portfolio, taking into account the attrition risk. CLV depends both on the purchase recency as well as on the purchasing rate and aims at identifying the most valuable customer groups. Formally, Gupta and Lehmann (Gupta and Lehmann 2003) define CLV for customer \\(i\\) as follows: \\[CLV_i = \\sum_{t=0}^{T_i} \\frac{(p_t - c_t)r_{i,t}}{(1+a)^t} - AC_i\\] with, \\(p_t\\) the price paid by customer \\(i\\) at time \\(t\\) \\(c_t\\) the cost at time \\(t\\) \\(r_{i,t}\\) the probability that customer \\(i\\) be active at time \\(t\\) \\(a\\) the discount rate \\(AC_i\\) the acquisition cost of customer \\(i\\) \\(T_i\\) the duration of observation An estimation of the portfolios overall value can be calculated through customer equity (CE) which amounts to the sum of all the CLVs. Since CE appears to be a good proxy of the firms value, the profit maximization program can be rewritten as: \\[ \\begin{aligned} \\max_{p} \\quad &amp; \\textrm{CE} = \\sum_{i=1}^{N} \\sum_{t=0}^{T_i} \\frac{(p_t - c_t)r_{i,t}}{(1+a)^t} - AC_i\\\\ \\textrm{s.t.} \\quad &amp; r_{i,t} \\in [0, 1]\\\\ &amp;p_t &gt; c_t \\\\ \\end{aligned} \\] References "],["2-literature.html", "Chapter 2 Literature Review", " Chapter 2 Literature Review Now the concepts of portfolio, attrition and value have been defined, it seems relevant to take a look at the literature on these notions. The literature review made in this chapter synthesises and analyses the available articles related to customer portfolio modelling, churn analysis as well as customer value estimation. The review combines concepts from Economics, Econometrics and Data Science. "],["2.1-portfolio.html", "2.1 On customer portfolio", " 2.1 On customer portfolio Over the course of time, there has been a slight amendment on the area of use of this term. At the outset, this term was principally used in finance (Markowitz 1952). As it went along, this expression is now employed when referring to the CRM (Homburg &amp; Al 2009). Customer portfolio analysis enables managers and researchers to capture a customers value input to a firms portfolio of links rather than analyzing a customers value to the firm singly. This term is spread over three distinct ways of treating it. For one thing, the most obvious thing might be to cleave the portfolio. What exactly does that mean? Well, the idea is quite clear. The authors consider a 4-branch segmented portfolio: Platinium Gold Silver Bronze With two issues: Cost to serve a client Value of this client The holy grail for a company would be to segment their clients in line with the value they give rise to. One question arises: Would companies treat customer segments differently if they could quantify their current and potential value? At first glance, one might think firms would enjoy having only platinium customers. This seems rational and cogent. Nonetheless, and this also acts as the second way of treatment, in a pre-release version of an article (Alain Bousquet 2021), the writer intends a completely novel and previously unseen idea of estimating a portfolio value. His idea is to rest no longer on the expected value of a portfolio, but rather on its variance. The brief conclusion that comes out is that it is preferable for the portfolio value to have some level of variance. Determining such a level could prove to be somewhat tricky. However the hunch behind that is easily understandable. A firm needs heterogeneous clients so as to properly segment them. Thus, she would be able to able to offer a great range of services. Last but not least, both cases above are studied following a static way. A huge intake would be the dynamisation of the problem. As stated by (Christian Homburg, Viviana V. Steiner, &amp; Dirk Totzek 2009), the value a customer has is prone to changes. Let us take the example of a teenager in a telecom company. It is likely he begins with a small monthly fee without lots of features. Nevertheless, when he gets older he will drastically change his consumption. The dynamics of a client are represented by Markov chains (insérer image). Therefor, firms try to estimate churn and status change probabilities over time. Dynamic portfolios get rid of one main concern about static portfolios: its annoying habit of underestimating low-value clients and overestimating high-value clients. Figure 2.1: A timeline on the concept of portfolio "],["2.2-attrition.html", "2.2 On attrition", " 2.2 On attrition Attrition or churn has become a buzzword these last years. It has sailed among different waterways: econometrics, machine learning Naturally enough, churn analysis has begun with the precious help of econometrics. Churn analysis can be seen as an economic problem. Indeed, churners are really frequent on oligopolistic markets. Moreover, as (Ana Maria Perez Marin 2004) states, there is an asymmetrical information between potential churners (clients) and firms. Her paper aims at estimating the life expectancy of a customer in a firms portfolio. Let us note her article is based on the insurance market, which has some peculiarities. She introduces a new non-parametric method to estimate customer lifetime duration. Subsequently, there are some new survival analysis tools in the field of ML. As read on (Carolina Bellani 2019) The same market is kept: the insurance one. She uses many ML models, among which random forests, logistic regression, neural networks The data preprocessing she makes is heavy. Indeed, she deals with unbalanced data and thus resorts to undersampling. This problem is common in ML, and it impacts negatively the estimation coherence. In order to get a robust enough model, one has to pick the best classification threshold according to one metric which bears in mind classification mistakes in both classes. Ultimately, when a customers churns out of a firm, it may worth it to consider all possible outcomes for the reason he churned. For instance, a client might leave his telecom company because of the lack of services, or because of a terrible 5G reception. Time and time again, there are many sakes possible for the client to leave. That brings on the table our final topic: competing risks. The focus is put on (Dorenda Slof, Flavius Frasincar,Vladyslav Matsiiako 2021). The main interest of competing risk analysis is to determine the reason why the client would churn, and to prevent the churn by operating directly on this problem. They introduce two methods in the article. The first one being the incorporation of text-based data on duration models. For example, they look carefully at the emails sent by the customer to the customer service. By the way, this might be a chance to develop NLP models to try preventing the churn. Then, the second one is to analyse clients requests, classify them and then stir them in the blender of features, which allows the models gaining performance and reliability. "],["2.3-value.html", "2.3 On customer value", " 2.3 On customer value In recent years, customer portfolio management (CPM) has focused on optimizing clients value to the firm. The companys interest lies in knowing how much net benefit it can expect from a customer today. These expectations are then used to implement efficient marketing strategies to get the highest return on investment. To that end, two key metrics are estimated by firms, namely customer lifetime value (CLV) and customer equity (CE) (see part 2.3 in the introduction for definitions). According to Blattberg and Deighton, CLV can be defined as the revenues derived from a customer minus the cost to the firm for maintaining the relationship with the customer over time (Blattberg and Deighton 1996). As shown by Reinartz and Kumar, CLV modelling depends on the type of relationship a firm has with its clients (Reinartz and Kumar. 2003). In a contractual relationship, customer defections are observed which means that longer lifetime means higher customer value. Conversely, when the relationship is non-contractual, uncertainty arises between the customers purchase behavior and lifetime. With the development of data collection tools, companies have lots of customer-level data (or customer transaction data) at their disposal to measure CLV (Fader and al. 2005). Consequently, different modelling approaches can be adopted in order to estimate customers value. RFM (Recency Frequency Monetary) models are considered the simplest strategy to increase CLV and customers loyalty (Gupta, Hanssens, and Hardie 2006). It aims at targeting specific marketing campaigns at specific groups of customers to improve response rates. RFM models consists in creating clusters of clients based on three variables: recency which is the time that has elapsed since a customers last activity with the firm; frequency that is the number of times a customer transacted with the brand in a particular period of time; monetary value of customers prior purchases. However, RFM models have a limited predictive power since they only predict clients behavior for the next period. In their article on CLV management, Borle and Singh draw the review of more advanced modelling techniques that can be implemented to estimate customers value (Borle and Singh 2008). A popular method to estimate customer lifetime value is the negative binomial distribution (NBD) - Pareto (Fader and al. 2005) which helps solving the lifetime uncertainty issue. The model takes past customer purchase behavior as input such as the number of purchases in a specific time window and the date of last transaction. Then the model outputs a repurchase probability as well as a transaction forecast for each individual. In Borle and Singhs research paper, a hierarchical bayesian model is implemented with a view to jointly predict customers churn risk and spending pattern. Here, the main advantage of using a bayesian approach is to give priors on CLVs drivers. The study is base on data coming from a membership-based direct marketing company where firm/client relationships are non-contractual. In other words, the times of each customer joining the membership and terminating it are known once these events happen. Thus the implementation of a sophisticated estimation strategy is justified. In our study, emphasize is placed on estimating the overall value of a customer portfolio. The methodology we will develop is based on a research paper written by our Econometrics teacher Alain Bousquet, whose goal is provide tools for an efficient management of a patent portfolio (Bousquet 2021). The main idea is to consider each patent as an asset with a related value which can generate income if this very patent is exploited. This modelling approach can be transposed to customer portfolio analysis with the customers value corresponding to the CLV and the probability of exploitation being the opposite of the risk of attrition. In this context, CLV can be estimated with techniques mentioned above. The clients risk of churn can be modelled with duration models or machine learning techniques as evoked in 2.2. References "],["3-duration.html", "Chapter 3 Duration models", " Chapter 3 Duration models This chapter presents theoretical basis of the models that are used to model customer portfolios. As a customers lifetime in a portfolio is usually represented by the time to churn, duration models are adapted to the data we have at our disposal. Thus, this part focuses on introducing standard survival techniques. "],["3.1-definition.html", "3.1 Definition", " 3.1 Definition According to Cameron &amp; Trivedi (Cameron and Trivedi 2005), duration models (also called survival models) aims at measuring the time spent in a certain state before transitioning to another state. In Econometrics, a state corresponds to the class in which an individual \\(i\\) is at time \\(t\\). a transition is movement from one state to another. a duration measures the time spent in a certain state and is also called a spell length. Since measuring the time until the event is needed for multiple purposes, duration analysis is used in a variety of economic sectors as depicted by the following table. Economic sector Purpose Macroeconomics Length of an unemployment spell Insurance Risk analysis to offer a segmented pricing Engineering Time until a device breaks down Epidemiology Survival time of a virus Churn analysis Time until a customer leave the portfolio References "],["3.2-censoring-and-truncation.html", "3.2 Censoring and Truncation", " 3.2 Censoring and Truncation When dealing with survival data, some observations are usually censored meaning they are related to spells which are not completely observed. Duration data can also suffer from a selection bias which is called truncation. 3.2.1 Censoring mechanisms Left-censoring occurs when the event of interest occurs before the beginning of the observation period. For example, an individual is included in a study of duration of unemployment at \\(t_0\\). At that time he has already been unemployed for a period but he cannot recall exactly the duration of this period. If we observe that he finds a job again at \\(t_1\\), we can only deduce that the duration of unemployment is bigger than \\(t_1-t_0\\), this individual is left-censored. Observation 2 on figure 3.1 is associated with a left-censored spell (Liu 2019). A spell is considered right-censored when it is observed from time \\(t_0\\) until a censoring time \\(t_c\\) as illustrated by observation 1 on figure 3.1. For instance, the lifetime related to a customer who has not churned at the end of the observation period is right-censored. Let us note \\(X_i\\) the duration of a complete spell and \\(C_i\\) the duration of a right-censored spell. We also note \\(T_i\\) the duration actually observed and \\(\\delta_i\\) the censoring indicator such that \\(\\delta_i = 0\\) if the spell is censored. Then, \\((t_1, \\delta_1),\\dots,(t_N, \\delta_N)\\) are the realizations of the following random variables: \\[\\begin{equation} \\begin{aligned} T_i &amp; = \\min(X_i, C_i) \\\\ \\delta_i &amp; = \\mathbb{1}_{X_i &lt; C_i} \\end{aligned} \\tag{3.1} \\end{equation}\\] 3.2.2 Selection bias Survival data suffers from a selection bias (or truncation) when only a sub-sample of the population of interest is studied. A customer entering the firms portfolio after the end of the study is said to be right-truncated, whereas a client who has left the portfolio before the beginning of the study is considered left-truncated. Mathematically, a random variable \\(X\\) is truncated by a subset \\(A \\in \\mathbb{R}^+\\) if instead of \\(\\Omega(X)\\), we solely observe \\(\\Omega(X)\\bigcap A\\). On figure 3.1, the first and fifth observations suffers from a selection bias. Figure 3.1: Censored and truncated data References "],["3.3-probabilistic-concepts.html", "3.3 Probabilistic concepts", " 3.3 Probabilistic concepts In survival analysis, the response variable denoted \\(T\\) is a time-to-event variable. Instead of estimating the expected failure time, survival models estimate the survival and hazard rate functions which depend on the realization of \\(T\\). 3.3.1 Survival function The survival function \\(S(t)\\) represents the probability that the considered event occurs after time \\(t\\). For instance, \\(S(t)\\) can measure the probability that a given customer survive in the portfolio at least until time \\(t\\). Mathematically, the survival function is defined as: \\[\\begin{equation} S(t) = P(T &gt; t) = 1 - F(t) \\tag{3.2} \\end{equation}\\] where \\(F(t)\\) is the cumulative distribution function. Figure 3.2: Survival function \\(S_T(t)\\) with \\(T \\sim \\mathcal{E} (1)\\) 3.3.2 Hazard and Cumulative Hazard functions Another key concept in duration analysis is the hazard function \\(\\lambda(t)\\) which approximates the probability that the event occurs at time \\(t\\). For instance, \\(\\lambda(t)\\) can measure the probability that a given individual leaves the firm portfolio at time \\(t\\). Formally, it is expressed as follows: \\[\\begin{equation} \\lambda(t) = \\lim_{\\Delta t \\to 0} \\frac{P\\big[t \\leq T &lt; t + \\Delta t | T \\geq t \\big]}{\\Delta t} \\tag{3.3} \\end{equation}\\] Using the Bayes formula, equation (3.3) can also be written as (see equation (5.1) in appendix): \\[\\begin{equation} \\lambda(t) = \\frac{-\\text{d} \\ln \\big(S(t)\\big)}{\\text{d} t} \\tag{3.4} \\end{equation}\\] Finally, integrating the instantaneous hazard function gives the cumulative hazard function which can be more precisely estimated than the hazard function (Cameron and Trivedi 2005) and is defined as: \\[\\begin{equation} \\Lambda (t) = \\int_{0}^{t} \\lambda(s) \\text{d}s = \\ln \\big(S(t)\\big) \\tag{3.5} \\end{equation}\\] Figure 3.3: Cumulative Hazard function \\(\\Lambda_T(t)\\) with \\(T \\sim \\mathcal{E} (1)\\) Thus, the hazard, survival and cumulative hazard functions are three mathematical functions which describe the same distribution. References "],["3.4-nonparam.html", "3.4 Nonparametric models", " 3.4 Nonparametric models When dealing with duration data, these methods are helpful to have a general overview of the raw (unconditional) hazard. Nonparametric models are rather used for data description rather than prediction. No explanatory variable is included in these models except for treatment variables such as the type of contract a customer has subscribe. 3.4.1 Notations Let us consider a sample with \\(N\\) observations with \\(k\\) discrete failure time (e.g. a failure can be a churn event), such that \\(\\forall j \\in [\\![1; k]\\!]\\) : \\(t_j\\) the \\(j^{\\text{th}}\\) discrete failure time, \\(d_j\\) the number of spells terminating at \\(t_j\\), \\(m_j\\) the number of right-censored spells in the interval \\([t_j, t_{j+1}]\\), \\(r_j\\) the number of exposed durations right before time \\(t_j\\) such that: \\[\\begin{equation} r_j = (d_j + m_j) + \\dots + (d_k + m_k) = \\sum_{l|l \\geq j} (d_l + m_l) \\tag{3.6} \\end{equation}\\] 3.4.2 Hazard function estimator As the instantaneous hazard at time \\(t_j\\) is defined as \\(\\lambda_j = P[T=t_j|T\\geq t_j]\\), a trivial estimator of \\(\\lambda_j\\) is obtained by dividing the number of durations for which the event is realized in \\(t_j\\) by the total number of exposed durations at time \\(t_j^{-}\\). Formally, it is expressed as: \\[\\begin{equation} \\hat{\\lambda}_j = \\frac{d_j}{r_j} \\tag{3.7} \\end{equation}\\] 3.4.3 Kaplan-Meier estimator Once the hazard function estimator computed, the discrete-time survivor function can be estimated using the Kaplan-Meier product-limit estimator. To estimate the survival at time \\(t\\), this estimator computes the joint probability that a spell stays in the same state until \\(t\\) (e.g. remaining loyal to a firm until a certain time). This method is based on conditional probabilities and the survival function estimate is defined as: \\[\\begin{equation} \\hat{S}(t) = \\Pi_{j|t_j \\leq t} \\big(1-\\hat{\\lambda}_j\\big) = \\Pi_{j|t_j \\leq t}\\frac{r_j - d_j}{r_j} \\tag{3.8} \\end{equation}\\] When plotting the survival curve after having performed the Kaplan-Meier estimation, confidence bands are also added to the plot in order to reflect sampling variability (Cameron and Trivedi 2005). The confidence interval of the survival function \\(S(t)\\) is derived from the estimate of the variance of \\(S(t)\\) which is obtained by the Grenwood estimate (see equation (3.9)). \\[\\begin{equation} \\hat{\\mathrm{V}}[\\hat{S}(t)] = \\hat{S}(t)^2 \\sum_{j|t_j \\leq t} \\frac{d_j}{r_j(r_j-d_j)} \\tag{3.9} \\end{equation}\\] 3.4.4 Nelson-Aalen estimator The cumulative hazard function estimate is given by the Nelson-Aalen estimator which consists in summing up the hazard estimates for each failure time. \\[\\begin{equation} \\hat{\\Lambda}(t) = \\sum_{j | t_j \\leq t} \\hat{\\lambda}_{j} = \\sum_{j | t_j \\leq t} \\frac{d_j}{r_j} \\tag{3.10} \\end{equation}\\] Exponentiating \\(\\hat{\\Lambda}(t)\\), one can obtain a second estimate of the survival function (see equation (5.2) in the appendix): \\[\\begin{equation} \\tilde{S}(t) = \\exp \\big( -\\hat{\\Lambda}(t) \\big) \\tag{3.11} \\end{equation}\\] References "],["3.5-parametric-models.html", "3.5 Parametric models", " 3.5 Parametric models The nonparametric estimation is undoubtedly useful when it comes to have a general overview on the survival data. However, one may want to model the hazard and survivor functions with a functional form in which unknown parameters need to be optimized. When implementing parametric models, \\(\\lambda\\), \\(S\\) and \\(\\Lambda\\) are expressed based on the chosen parametric form. The instantaneous hazard function can either be constant or monotone. In our study we assume that the explanatory variables are time-constant as we do not have dynamic data at our disposal. Parametric estimation has a twofold purpose that is to implement a robust model to estimate the risk that a specific event occurs while identifying the variables (or covariates) which best explain this risk. 3.5.1 Constant hazard (exponential model) The exponential distribution models the time between events in a Poisson process and has the key property of being memoryless. Let us note \\(T\\) a time-to-event variable following such that \\(T \\sim \\mathcal{E}(\\theta)\\) where \\(\\theta\\) is the rate parameter. In this context, memorylessness can be defined as follows: \\[\\begin{equation} \\mathbb{P}(T&gt;t+s | T &gt; t) = \\mathbb{P}(T&gt;s) \\tag{3.12} \\end{equation}\\] \\(\\forall t \\geq 0\\ ,\\ \\theta &gt; 0\\) the density, hazard and survival functions can be expressed as: \\[\\begin{equation} \\begin{aligned} f_{\\theta}(t) &amp; = \\theta e^{-\\theta t} \\\\\\\\ \\lambda_{\\theta}(t) &amp; = \\theta \\\\\\\\ S_{\\theta}(t) &amp; = e^{-\\theta t} \\\\\\\\ \\end{aligned} \\tag{3.13} \\end{equation}\\] Thus, the exponential distribution is characterized by a constant hazard function which is a consequence of memorylessness. 3.5.2 Monotone hazard Weibull model The Weibull distribution is a less restrictive generalization of the exponential distribution defined by a shape parameter \\(\\nu\\) and a scale parameter \\(\\theta\\). \\(\\forall t \\geq 0\\) et \\(k=\\nu,\\ \\theta &gt; 0\\) the density, hazard and survival functions can be expressed as: \\[\\begin{equation} \\begin{aligned} \\lambda_{k,\\theta}(t) &amp; = \\nu \\bigg(\\frac{1}{\\theta}\\bigg)^{\\nu} t^{\\nu - 1} \\\\\\\\ S_{k, \\nu}(t) &amp; = \\exp \\Bigg( -\\bigg(\\frac{1}{\\theta}\\bigg)^{\\nu} t\\Bigg) \\end{aligned} \\tag{3.14} \\end{equation}\\] The instantaneous hazard function \\(\\lambda_{\\nu,\\theta}\\) is: monotonic decreasing if \\(\\nu \\in [0, 1]\\) (e.g. customer loyalty), constant if \\(\\nu=1\\) and \\(T \\sim \\mathcal{E}(\\theta)\\), monotonic increasing if \\(\\nu &gt; 1\\) (e.g. customers leaving the portfolio to a competing firm). Figure 3.4 illustrate the hazard and survivor functions associated to a variable \\(T\\) which follows a Weibull distribution. The two curves shape depend both on the shape (\\(\\nu\\)) and scale parameters (\\(\\theta\\)). Some remarks can be made looking at the two plots. On the one hand, the higher \\(\\nu\\), the more increasing the hazard function. Furthermore, when \\(\\nu &lt; 1\\) the hazard function is decreasing meaning that the risk of the event occurring decreases as time goes by. When \\(\\nu &gt; 1\\) the hazard function is convex which indicates that a marginal increase in time leads to an increase (decrease) of over one unit in the the hazard (survivor) function. On the other hand, it can be noted that the Weibull distribution corresponds to the exponential distribution when \\(\\nu = \\theta = 1\\) (see figures 3.2 and 3.3). Figure 3.4: Hazard and Survival functions with \\(T \\sim \\mathcal{W} (\\nu, \\theta)\\) Other models Different probabilistic distributions can be chosen the model the hazard and survival functions related to a time-to-event variable. The Gompertz model is usually used for mortality data in biostatistics. The gamma model depends both on the gamma and inverse-gamma distribution and is also based on shape and scale parameters. 3.5.3 Concave and convex hazard When the hazard related to the studied phenomenon varies, the distributions introduced above are limited as they model monotone hazard. The generalized Weibull model appears to be a good choice to estimate phenomena with concave or convex hazard. It is based on three parameters: \\(\\nu\\) (shape), \\(\\theta\\) (scale) and \\(\\gamma\\). When \\(\\gamma = 1\\), the generalized Weibull becomes the Weibull distribution \\(\\mathcal{W}(\\nu, \\theta)\\). "],["3.6-semi-parametric-estimation.html", "3.6 Semi-parametric estimation", " 3.6 Semi-parametric estimation 3.6.1 Proportional Hazards models Parametric models assume that the baseline (or raw) hazard belongs to a specific distribution. This assumption can be sometimes too restrictive and semi-parametric models can be more adapted to describe the duration data. In proportional hazards (PH) models, the instantaneous risk function is to the baseline hazard \\(\\lambda_0 (t,\\alpha)\\) modulo a depending on the covariates \\(\\phi(\\pmb{\\mathrm{x}}, \\beta)\\). These models allow to generalize the basic survival models to a survival regression model which permits to take individuals heterogeneity into consideration (Harrell 1984). The general mathematical formulation is expressed as follows: \\[\\begin{equation} \\lambda(t|\\pmb{\\mathrm{x}}) = \\lambda_0 (t,\\alpha) \\phi(\\pmb{\\mathrm{x}}, \\beta) \\tag{3.15} \\end{equation}\\] Note that when the function form of \\(\\lambda_0 (t,\\alpha)\\) is known, we are in the case of parametric estimation. For instance, the exponential, Weibull and Gompertz models are PH models since their respective hazards are function of some covariates. What does proportional hazards mean? PH models are said to be proportional as the relative hazard ratio between two individuals \\(i\\) and \\(k\\) does not vary over time, such that: \\[\\begin{equation} \\frac{\\lambda(t|\\pmb{\\mathrm{x}_i})}{\\lambda(t|\\pmb{\\mathrm{x}_k})} = \\frac{\\phi(\\pmb{\\mathrm{x}_i}, \\beta) }{\\phi(\\pmb{\\mathrm{x}_k}, \\beta)} \\tag{3.16} \\end{equation}\\] The formulation stated in equation (3.16) needs to be verified when one wants to fit a PH model to real-life data. Marginal effects In proportional hazards models, the marginal effect of covariate \\(x_p\\) on the hazard function can be easily derived since this computation only requires knowledge on \\(\\beta\\). As shown in Cameron and Trivedi (2005), a one-unit increase in the \\(p^{\\text{th}}\\) covariate leads to the following variation in the hazard function ceteris paribus: \\[\\begin{equation} \\frac{\\partial \\lambda(t|\\pmb{\\mathrm{x}}, \\beta)}{\\partial x_p} = \\lambda(t|\\pmb{\\mathrm{x}}, \\beta) \\frac{\\partial \\phi(\\pmb{\\mathrm{x}}, \\beta) / \\partial x_p}{\\phi(\\pmb{\\mathrm{x}}, \\beta) } \\tag{3.17} \\end{equation}\\] Partial likelihood estimation The vector of parameters \\(\\beta\\) related to the regression part of the PH model is estimated by partial likelihood maximization. The methods principle consists in only estimating the regressions parameters \\(\\beta\\) by considering the baseline hazard \\(\\lambda_0\\) as noise. If desired an estimate of the baseline hazard can be recovered after estimation of \\(\\beta\\) using for instance, the Nelson-Aalen estimator (see 3.4). Coxs intuition is that no information can be retrieved from the intervals during which no event has occurred and that it is conceivable that \\(\\lambda_0\\) is null in these intervals. Thus, solely the set of moments when an event occurs are considered in the estimation method. In order to derive the partial likelihood function, let us note \\(t_j\\) the \\(j^{\\text{th}}\\) discrete failure time in an \\(N\\)-sample with \\(j \\in [\\![1; k]\\!]\\), such that: \\(t_1 &lt; t_2 &lt; \\dots &lt; t_k\\), \\(D(t_j) = \\{l: t_l = t_j\\}\\) is the set of spells completed at \\(t_j\\) with \\(\\#D(t_j) = d_j\\), \\(R(t_j) = \\{l: t_l \\geq t_i\\}\\) is the set of spells at risk at \\(t_j\\). The contribution of a spell in \\(D(t_j)\\) to the likelihood function equals the conditional probability that the spell ends at \\(t_j\\) given it is exposed at that specific time and can be written as (see Cameron and Trivedi (2005) and proof (5.3) for more details): \\[\\begin{equation} \\mathbb{P}\\big[T_j = t_j | R(t_j) \\big] = \\frac{\\phi(\\pmb{\\mathrm{x}_j}, \\beta)}{\\sum_{l \\in R(t_j)} \\phi(\\pmb{\\mathrm{x}_l}, \\beta)} \\tag{3.18} \\end{equation}\\] Given \\(k\\) discrete failure times are considered and that for each of those there is a set \\(D(t_j)\\) of completed spells, Cox defines the partial likelihood function as the joint product of the probability expressed in (3.18), such that: \\[\\begin{equation} \\mathcal{L}_p = \\Pi_{j=1}^{k} \\ \\frac{\\Pi_{m \\in D(tj)} \\ \\phi(\\pmb{\\mathrm{x}_j}, \\beta)}{\\Big[\\sum_{l \\in R(t_j)} \\phi(\\pmb{\\mathrm{x}_l}, \\beta)\\Big]^{d_j}} \\tag{3.19} \\end{equation}\\] The latter formulation of the partial likelihood function is explained in more details in equations (5.4) and (5.5) in the appendix. 3.6.2 Cox PH model The Cox proportional hazards model is the most popular for the analysis of duration data. This model is said to be semi-parametric as makes no assumption regarding the nature of the baseline hazard function \\(\\lambda_0(t)\\). The parametric part only relies in the modelling of the effect of some covariates on the hazard function \\(\\lambda(t)\\). The relationship between the vector of covariates and the log hazard is linear and the parameters can be estimated by maximizing the partial likelihood function. The Cox PH model solely assumes that predictors act multiplicatively on the hazard function. The model is formulated as in equation (3.15) with the exponential function as link function between the hazard and the covariates i.e. \\(\\lambda(t|\\pmb{\\mathrm{x}}) = \\lambda_0 (t,\\alpha) \\text{e}^{\\pmb{\\mathrm{x&#39;}} \\beta}\\). References "],["4-ml.html", "Chapter 4 Machine Learning techniques", " Chapter 4 Machine Learning techniques A computer program is said to learn from experience \\(E\\) with respect to some class of tasks \\(T\\) and performance measure \\(P\\) if its performance at tasks in \\(T\\), as measured by \\(P\\), improves with experience \\(E\\).  Tom M. Mitchell  "],["5-data.html", "Chapter 5 Data", " Chapter 5 Data In this chapter, we introduce the kaggle dataset related to customers of a fictional telecommunications service provider (TSP). "],["appendix.html", "Appendix", " Appendix In this part, some proofs of the mathematical concepts used in the study are derived. "],["duration-models.html", "Duration models", " Duration models Hazard function \\[\\begin{equation} \\begin{aligned} \\lambda(t) &amp; = \\lim_{\\Delta t \\to 0} \\frac{P\\big[t \\leq T &lt; t + \\Delta t | T \\geq t \\big]}{\\Delta t} \\\\\\\\ &amp; = \\lim_{\\Delta t \\to 0} \\frac{P\\big[t \\leq T &lt; t + \\Delta t \\big] / P\\big[T \\geq t \\big]}{\\Delta t} \\\\\\\\ &amp; = \\lim_{\\Delta t \\to 0} \\frac{\\big(F(t+\\Delta t)-F(t)\\big) / \\Delta t}{S(t)} \\\\\\\\ &amp; = \\frac{\\text{d} F(t) / \\text{d} t}{S(t)} \\\\\\\\ &amp; = \\frac{f(t)}{S(t)} \\\\\\\\ \\lambda(t) &amp; = \\frac{-\\text{d} \\ln \\big(S(t)\\big)}{\\text{d} t} \\end{aligned} \\tag{5.1} \\end{equation}\\] Link between cumulative hazard and survivor functions \\[\\begin{equation} \\begin{aligned} &amp; \\Lambda(t) = \\int_{0}^{t} \\lambda(s)ds \\\\\\\\ \\iff &amp; \\Lambda(t) = \\int_{0}^{t} \\frac{f(s)}{S(s)}ds \\\\\\\\ \\iff &amp; \\Lambda(t) = -\\ln \\big(S(t)\\big) \\\\\\\\ \\iff &amp; S(t) = \\exp \\big(-\\Lambda(t)\\big) \\end{aligned} \\tag{5.2} \\end{equation}\\] Contribution to the partial likelihood function in PH models \\[\\begin{equation} \\begin{aligned} \\mathbb{P}\\big[T_j = t_j | R(t_j) \\big] &amp; = \\frac{\\mathbb{P}\\big[T_j = t_j | T_j \\geq t_j \\big]}{\\sum_{l \\in R(t_j)} \\ \\mathbb{P}\\big[T_l = t_l | T_l \\geq t_j \\big]} \\\\\\\\ &amp; = \\frac{\\lambda_j(t_j|\\pmb{\\mathrm{x_j}}, \\beta)}{\\sum_{l \\in R(t_j)} \\ \\lambda_l(t_l|\\pmb{\\mathrm{x_l}}, \\beta)} \\\\\\\\ &amp; = \\frac{\\lambda_0 (t_j, \\alpha)\\phi(\\pmb{\\mathrm{x_j}}, \\beta)}{\\sum_{l \\in R(t_j)} \\ \\lambda_0 (t_j, \\alpha)\\phi(\\pmb{\\mathrm{x_l}}, \\beta)} \\\\\\\\ \\mathbb{P}\\big[T_j = t_j | R(t_j) \\big] &amp; = \\frac{\\phi(\\pmb{\\mathrm{x}_j}, \\beta)}{\\sum_{l \\in R(t_j)} \\phi(\\pmb{\\mathrm{x}_l}, \\beta)} \\end{aligned} \\tag{5.3} \\end{equation}\\] Partial likelihood function in PH models Based on equation (3.18), one can derive the probability that all spells completed at \\(t_j\\) ends in the \\(j^{\\text{th}}\\) failure time, such that: \\[\\begin{equation} \\begin{aligned} \\mathcal{L}_{p,\\ t_j} &amp; = \\mathbb{P}\\big[T_1 = t_j, \\dots, T_{d_j} = t_j \\ | \\ R(t_j)\\big] \\\\\\\\ &amp; = \\Pi_{m \\in D(t_j)} \\ \\mathbb{P}\\big[T_m = t_j | R(t_j) \\big] \\\\\\\\ &amp; = \\Pi_{m \\in D(t_j)} \\ \\frac{\\phi(\\pmb{\\mathrm{x}_m}, \\beta)}{\\sum_{l \\in R(t_j)} \\phi(\\pmb{\\mathrm{x}_l}, \\beta)} \\\\\\\\ &amp; = \\Pi_{m \\in D(t_j)} \\ \\phi(\\pmb{\\mathrm{x}_m}, \\beta) \\times \\Pi_{m \\in D(t_j)} \\ \\frac{1}{\\sum_{l \\in R(t_j)} \\phi(\\pmb{\\mathrm{x}_l}, \\beta)} \\\\\\\\ \\mathcal{L}_{p,\\ t_j} &amp; = \\frac{\\Pi_{m \\in D(t_j)} \\ \\phi(\\pmb{\\mathrm{x}_m}, \\beta)}{\\Big[\\sum_{l \\in R(t_j)} \\phi(\\pmb{\\mathrm{x}_l}, \\beta)\\Big]^{d_j}} \\end{aligned} \\tag{5.4} \\end{equation}\\] The joint probability over the \\(k\\) ordered discrete failure times then becomes: \\[\\begin{equation} \\begin{aligned} \\mathcal{L}_p &amp; = \\Pi_{j=1}^{k} \\ \\mathcal{L}_{p,\\ t_j} \\\\\\\\ \\mathcal{L}_p &amp; = \\Pi_{j=1}^{k} \\ \\frac{\\Pi_{m \\in D(t_j)} \\ \\phi(\\pmb{\\mathrm{x}_m}, \\beta)}{\\Big[\\sum_{l \\in R(t_j)} \\phi(\\pmb{\\mathrm{x}_l}, \\beta)\\Big]^{d_j}} \\end{aligned} \\tag{5.5} \\end{equation}\\] "],["references.html", "References", " References "]]
